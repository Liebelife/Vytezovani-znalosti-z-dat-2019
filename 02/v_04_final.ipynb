{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Úkol č. 2 - předzpracování dat a binární klasifikace (do 20. dubna)\n",
    "\n",
    "  * Cílem tohoto úkolu je vyzkoušet si naučit prediktivní model pro binární klasifikaci.\n",
    "  * Budete se muset vypořádat s příznaky, které jsou různých typů a které bude třeba nějakým způsobem převést do číselné reprezentace.\n",
    "    \n",
    "> **Úkoly jsou zadány tak, aby Vám daly prostor pro invenci. Vymyslet _jak přesně_ budete úkol řešit, je důležitou součástí zadání a originalita či nápaditost bude také hodnocena!**\n",
    "\n",
    "## Zdroj dat\n",
    "\n",
    "Budeme se zabývat predikcí přežití pasažérů Titaniku.\n",
    "K dispozici máte trénovací data v souboru **data.csv** a data na vyhodnocení v souboru **evaluation.csv**.\n",
    "\n",
    "#### Seznam příznaků:\n",
    "* survived - zda přežil, 0 = Ne, 1 = Ano, **vysvětlovaná proměnná**, kterou chcete predikovat\n",
    "* pclass - Třída lodního lístku, 1 = první, 2 = druhá, 3 = třetí\n",
    "* name - jméno\n",
    "* sex - pohlaví\n",
    "* age - věk v letech\n",
    "* sibsp\t- počet sourozenců / manželů, manželek na palubě\n",
    "* parch - počet rodičů / dětí na palubě\n",
    "* ticket - číslo lodního lístku\n",
    "* fare - cena lodního lístku\n",
    "* cabin\t- číslo kajuty\n",
    "* embarked\t- místo nalodění, C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "* home.dest - Bydliště/Cíl\n",
    "\n",
    "## Pokyny k vypracování\n",
    "\n",
    "**Základní body zadání**, za jejichž (poctivé) vypracování získáte **8 bodů**:\n",
    "  * V Jupyter notebooku načtěte data ze souboru **data.csv**. Vhodným způsobem si je rozdělte na trénovací, testovací a případně i validační množinu (preferujeme ale použití cross-validation).\n",
    "  * Projděte si jednotlivé příznaky a transformujte je do vhodné podoby pro použití ve vybraném klasifikačním modelu.\n",
    "  * Podle potřeby si můžete vytvářet nové příznaky (na základě existujících), například tedy můžete vytvořit příznak měřící délku jména. Některé příznaky můžete také úplně zahodit.\n",
    "  * Nějakým způsobem se vypořádejte s chybějícími hodnotami.\n",
    "  * Následně si vyberte vhodný klasifikační model z přednášek. Najděte vhodné hyperparametry a určete jeho přesnost (accuracy) na trénovací množině. Také určete jeho přesnost na testovací/vaidační množině.\n",
    "  * Načtěte vyhodnocovací data ze souboru **evaluation.csv**. Napočítejte predikce pro tyto data (vysvětlovaná proměnná v nich již není). Vytvořte **results.csv** soubor, ve kterém tyto predikce uložíte do dvou sloupců: ID, predikce přežití. Tento soubor nahrajte do repozitáře.\n",
    "\n",
    "**Další body zadání** za případné další body  (můžete si vybrat, maximum bodů za úkol je každopádně 12 bodů):\n",
    "  * (až +4 body) Aplikujte všechny klasifikační modely z přednášek a určete (na základě přesnosti na validační množině), který je nejlepší. Přesnost tohoto nejlepšího modelu odhadněte pomocí testovací množiny. K predikcím na vyhodnocovacích datech využijte tento model.\n",
    "  * (až +4 body) Zkuste použít nějaké (alespoň dvě) netriviální metody doplňování chybějících hodnot u věku. Zaměřte pozornost na vliv těchto metod na přesnost predikce výsledného modelu. K predikcím na vyhodnocovacích datech využijte ten přístup, který Vám vyjde jako nejlepší.\n",
    "  \n",
    " **jiné bonusové zadání (Druhá možnosť bonusových bodov ostáva rovnaká):** \n",
    "\n",
    "- Aplikujte všechny klasifikační modely z přednášek - rozhodovací strom, random forest, adaboost, KNN, logistická regresia\n",
    "- Pre učenie rôznych modelov a ich porovnávanie, je pre Vás, ale tak isto aj pre mňa ako opravujúceho jednoduchšie a prehladnejšie vytvoriť jednu funkciu. HINT: pomocou parametra môžete predávať aj model, ktorý chcete učiť.\n",
    "\t\n",
    "\n",
    "## Poznámky k odevzdání\n",
    "\n",
    "  * Řiďte se pokyny ze stránky https://courses.fit.cvut.cz/BI-VZD/homeworks/index.html.\n",
    "  * Odevzdejte nejen Jupyter Notebook, ale i _csv_ soubor(y) s predikcemi pro vyhodnocovací data.\n",
    "  * Opravující Vám může umožnit úkol dodělat či opravit a získat tak další body. **První verze je ale důležitá a bude-li odbytá, budete za to penalizováni**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nice overall description in detail, great visualisation:\n",
    "https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n",
    "\n",
    "nice machine learning algorithms explanation, more simple:\n",
    "https://www.kaggle.com/stuartwaller/lost-overwhelmed-start-here\n",
    "\n",
    "well structured, great visualisation:\n",
    "https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "\n",
    "family size:\n",
    "https://www.kaggle.com/lperez/titanic-a-deeper-look-on-family-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**useful code**:\n",
    "- shape(), head(), info(), describe(), nunique(), isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('evaluation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying both training and testing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, I join both datasets under a new variable name **full_data** (excluding the 'survived' column from training data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([train_df.drop('survived', axis=1),test_df], axis=0).reset_index(drop=True)\n",
    "# display(full_data.head())\n",
    "# full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I drop **ticket** and **home.dest** columns. I also decided to drop **'cabin'**, since one hot encoding would add too many dummies and could result in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "full_data = full_data.drop(['ticket', 'home.dest', 'cabin'], axis=1)\n",
    "# display(full_data)\n",
    "# full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting **title** from **name**. Since there are only 4 major categories **Mr, Miss, Mrs and Master** (Master - for male children, an old form of invitation, not used anymore), I decided to create a new category **Rare** for the remaining titles. I am performing conversion to dummy variables for these 5 categories. When finished, I drop the column **name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# name: extract title, drop name, one hot encoding for title\n",
    "\n",
    "full_data['title'] = None\n",
    "\n",
    "# title extraction\n",
    "for name in range(len(full_data)):\n",
    "        full_data['title'].iloc[name] = ((full_data.name.iloc[name].split(','))[1].split())[0].strip('.')\n",
    "        \n",
    "# ## display title categories\n",
    "# full_data.title.unique()\n",
    "\n",
    "# ## see frequency of occurences:\n",
    "# display(full_data.title.value_counts())\n",
    "\n",
    "# Mr, Miss, Mrs and Master - most common\n",
    "# other types -> new label 'Rare'\n",
    "for title in range(len(full_data.title)):\n",
    "    if full_data.title.iloc[title] in ['Mr', 'Miss','Mrs','Master']:\n",
    "        pass\n",
    "    else:\n",
    "        full_data.title.iloc[title] = 'Rare'\n",
    "# display(full_data.title.value_counts())\n",
    "\n",
    "# one hot encoding for title\n",
    "titles_dummies = pd.get_dummies(full_data.title, prefix='Title')\n",
    "full_data = pd.concat([full_data, titles_dummies], axis=1)\n",
    "\n",
    "# drop column name\n",
    "full_data = full_data.drop('name', axis=1)\n",
    "\n",
    "# display(full_data)\n",
    "# full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sex** from string to numeric values (female 1, male 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# sex: from string to numeric (female 1, male 0)\n",
    "full_data.sex.replace({'female': 1,'male': 0}, inplace=True)\n",
    "\n",
    "# display(full_data)\n",
    "# full_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fare**: fill missing data with median value + log() all values to reduce skewness of data. Visualisations:\n",
    "https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# fare: fill missing with median value\n",
    "\n",
    "## missing data:\n",
    "# display(full_data[full_data.fare.isnull()])\n",
    "\n",
    "# since only one fare value is missing, i decided to fill it with fare median value for passenger class 3 (since )\n",
    "pclass_fare = full_data.loc[:,['fare', 'pclass']][full_data.pclass==3]\n",
    "median = pclass_fare.describe().fare['50%']\n",
    "full_data.loc[:,'fare'][full_data.loc[:,'fare'].isnull()] = median\n",
    "\n",
    "# transforming fare data by log function to reduce skewness of data\n",
    "full_data.fare = full_data.fare.map(lambda i: np.log(i) if i > 0 else 0)\n",
    "# full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embarked**: fill missing values with median; converting all three (C, Q, S) to dummies. When finished, I drop the column 'embarked'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# embarked: fill missing values with median; dummies (one hot encoding) \n",
    "\n",
    "# two values missing:\n",
    "full_data.isnull().sum().embarked\n",
    "\n",
    "# S is the most common (689 / 998 values) - I'll assign this value to the missing values\n",
    "embarked = full_data.embarked.describe()\n",
    "full_data.loc[:,'embarked'][full_data.loc[:,'embarked'].isnull()] = embarked.top\n",
    "\n",
    "# convert column 'embarked' to dummies\n",
    "embarked_dummies = pd.get_dummies(full_data.embarked, prefix='Emb')\n",
    "full_data = pd.concat([full_data, embarked_dummies], axis=1)\n",
    "\n",
    "# drop embarked column\n",
    "full_data = full_data.drop(['embarked'], axis=1)\n",
    "\n",
    "# display(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  dealing with missing \"age\" values two ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <1st_way>: filling missing values based on *pclass, title, sibsp* and *parch* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**: filling missing values based on **pclass, title, sibsp** and **parch** data. Inspired by https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# visualisation\n",
    "train_df['title'] = full_data.title\n",
    "display(train_df.describe())\n",
    "g = sns.factorplot(y=\"age\",x=\"survived\",hue=\"parch\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"age\",x=\"survived\",hue=\"sibsp\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"age\",x=\"survived\",hue=\"title\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"age\",x=\"pclass\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"age\",x=\"title\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"age\",x=\"parch\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"age\",x=\"sibsp\", data=train_df,kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# age: fill missing values\n",
    "\n",
    "# taking indexes of rows with missing values in the column 'age'\n",
    "index_NaN_age = list(full_data[\"age\"][full_data[\"age\"].isnull()].index)\n",
    "\n",
    "# filling missing values\n",
    "for i in index_NaN_age :\n",
    "    age_med = full_data[\"age\"].median() # in case of no match\n",
    "    age_pred = full_data[\"age\"][((full_data['sibsp'] == full_data.iloc[i][\"sibsp\"])\n",
    "                                  & (full_data['parch'] == full_data.iloc[i][\"parch\"]) \n",
    "                                  & (full_data['pclass'] == full_data.iloc[i][\"pclass\"])\n",
    "                                  & (full_data['title'] == full_data.iloc[i][\"title\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        full_data['age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        full_data['age'].iloc[i] = age_med\n",
    "        \n",
    "# # check\n",
    "# full_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### </1st_way>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the **title** column as it is no longer needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.drop(['title'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sibsp and parch**: use both to determine family size and create **largeF** (large family) column with binary values (based on this kaggle article: https://www.kaggle.com/lperez/titanic-a-deeper-look-on-family-size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# sibsp and parch: use both to determine family size and create largeFamily column with binary values DONE\n",
    "\n",
    "# Create a family size column\n",
    "full_data[\"fsize\"] = full_data[\"sibsp\"] + full_data[\"parch\"] + 1\n",
    "\n",
    "# transforming values to binary: large family (5+ members) => 1, less => 0\n",
    "full_data.loc[full_data['fsize'] <= 4, 'fsize'] = 0\n",
    "full_data.rename(columns={'fsize':'largeF'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns **sibsp** and **parch** as they're no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.drop(['sibsp', 'parch'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <2nd_way>: filling missing \"age\" values with KNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.age.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copied from the lecture 5:\n",
    "\n",
    "The idea is this (assume we want to fill missing values in `Age` column):\n",
    "  * Split the dataset into two parts: \n",
    "    * `D1` = contaning the lines with missing values in `Age` column, \n",
    "    * `D2` = the rest of the data.\n",
    "  * Save the column `D2.Age` to `Y` and the remaining columns to `X` (exclude some columns if needed). The same columns of `D1` save to `X2`.\n",
    "  * Fit a model (we use the kNN) to predict `Y` using `X`.\n",
    "  * Use this model to predict the missing values of `Age` using the `X2` data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "D1 = full_data[full_data.age.isnull()]\n",
    "D2 = full_data[full_data.age.notnull()]\n",
    "\n",
    "Y = D2.age\n",
    "X = D2.drop([\"age\",\"ID\"], axis=1)\n",
    "X2 = D1.drop([\"age\",\"ID\"], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=30)\n",
    "knn.fit(X, Y)\n",
    "Y2 = knn.predict(X2)\n",
    "D1.age = Y2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "full_data = pd.concat([D1,D2], axis=0).sort_index()\n",
    "full_data.age.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  </2nd_way>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final product of modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# full_data.head()\n",
    "# full_data.describe()\n",
    "# full_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset division: train, evaluation + survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "train_data = full_data[:1000]\n",
    "train_data_no_ID = train_data.drop('ID', axis=1)\n",
    "survived = train_df.loc[:,'survived']\n",
    "\n",
    "# testing data\n",
    "eval_data = full_data[1000:]\n",
    "eval_no_ID = eval_data.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation\n",
    "\n",
    "scikit documentation\n",
    ">... training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "> However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets. \n",
    "> A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV.\n",
    "\n",
    "\\+ https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for trying different classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.metrics as confusion_matrix\n",
    "\n",
    "def model_data(model, param_grid, Xdata, ydata):\n",
    "    rd_seed = 333\n",
    "    test_size = 0.25\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(Xdata, ydata, test_size=test_size, random_state=rd_seed)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, cv = 10, n_jobs = -1)\n",
    "    grid_search.fit(Xtrain, ytrain)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(best_params)\n",
    "    print('accuracy score (train): {0:.6f}'.format(best_model.score(Xtrain, ytrain)))\n",
    "    print('accuracy score (test): {0:.6f}'.format(best_model.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for trying multiple classification models at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# function - multiple models\n",
    "def try_different_models(models_with_grids, Xdata, ydata):\n",
    "    '''requires 'models' in form of {model:{parameter_grid}}'''\n",
    "    for m, p in models_with_grids.items():\n",
    "        model = m\n",
    "        param_grid = p\n",
    "        print(str(model).split('(')[0],':')\n",
    "        model_data(model, param_grid, Xdata, ydata)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata = train_data_no_ID\n",
    "ydata = survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_params = {\n",
    "    DecisionTreeClassifier(random_state=42) : {'max_depth' : range(1,101),\n",
    "                              'criterion': ['entropy', 'gini']},\n",
    "    \n",
    "    RandomForestClassifier(random_state=42) : {'max_depth' : range(1,5), \n",
    "                              'n_estimators': range(1,60,2)},\n",
    "    \n",
    "    AdaBoostClassifier() : {'n_estimators': range(1,100,5),\n",
    "                          'learning_rate': [0.01, 0.05, 0.1, 0.3, 0.5, 1]},\n",
    "    \n",
    "    LogisticRegression() : {},\n",
    "    \n",
    "    KNeighborsClassifier() : {'n_neighbors': range(1,20),\n",
    "                            'p': range(1,5),\n",
    "                            'weights': ['uniform', 'distance']\n",
    "                           }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier :\n",
      "{'criterion': 'entropy', 'max_depth': 5}\n",
      "accuracy score (train): 0.833333\n",
      "accuracy score (test): 0.836000\n",
      "\n",
      "RandomForestClassifier :\n",
      "{'max_depth': 4, 'n_estimators': 13}\n",
      "accuracy score (train): 0.822667\n",
      "accuracy score (test): 0.836000\n",
      "\n",
      "AdaBoostClassifier :\n",
      "{'learning_rate': 0.1, 'n_estimators': 86}\n",
      "accuracy score (train): 0.808000\n",
      "accuracy score (test): 0.808000\n",
      "\n",
      "LogisticRegression :\n",
      "{}\n",
      "accuracy score (train): 0.810667\n",
      "accuracy score (test): 0.820000\n",
      "\n",
      "KNeighborsClassifier :\n",
      "{'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}\n",
      "accuracy score (train): 0.810667\n",
      "accuracy score (test): 0.808000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_different_models(models_with_params, Xdata, ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for evaluation.csv using chosen model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best model, parameters and missing \"Age\" values filling method\n",
    "Eventhough RandomForestClassifier with \"Age 2\" option of filling missing values reached the highest test accuracy (84 %), it was not the only information I took into account. The final decision is based on both test accuracy and the difference between test and train accuracy. That's why **I picked the DecisionTreeClassifier with option \"Age 1\"** as the \"best solution\".\n",
    "Overview of the accuracy scores for different models and missing Age values filling methods\n",
    " process is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score (train): 0.833333\n",
      "accuracy score (test): 0.836000\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "rd_seed = 333\n",
    "test_size = 0.25\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(Xdata, ydata, test_size=test_size, random_state=rd_seed)\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=5, criterion='entropy', random_state=42)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypredicted = model.predict(Xtest)\n",
    "\n",
    "print('accuracy score (train): {0:.6f}'.format(metrics.accuracy_score(ytrain, model.predict(Xtrain))))\n",
    "print('accuracy score (test): {0:.6f}'.format(metrics.accuracy_score(ytest, ypredicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation and writing into .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame()\n",
    "final['ID'] = eval_data.ID\n",
    "final['yeval'] = model.predict(eval_no_ID)\n",
    "final.reset_index(drop=True, inplace=True)\n",
    "final.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the accuracy scores for different models and missing Age values filling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier :\n",
    "{'criterion': 'entropy', 'max_depth': 5}\n",
    "accuracy score (train): 0.833333\n",
    "accuracy score (test): 0.836000\n",
    "\n",
    "RandomForestClassifier :\n",
    "{'max_depth': 4, 'n_estimators': 13}\n",
    "accuracy score (train): 0.822667\n",
    "accuracy score (test): 0.836000\n",
    "\n",
    "AdaBoostClassifier :\n",
    "{'learning_rate': 0.1, 'n_estimators': 86}\n",
    "accuracy score (train): 0.808000\n",
    "accuracy score (test): 0.808000\n",
    "\n",
    "LogisticRegression :\n",
    "{}\n",
    "accuracy score (train): 0.810667\n",
    "accuracy score (test): 0.820000\n",
    "\n",
    "KNeighborsClassifier :\n",
    "{'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}\n",
    "accuracy score (train): 0.810667\n",
    "accuracy score (test): 0.808000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGE 2 and k=5 for KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier :\n",
    "{'criterion': 'entropy', 'max_depth': 3}\n",
    "accuracy score (train): 0.810667\n",
    "accuracy score (test): 0.828000\n",
    "\n",
    "RandomForestClassifier :\n",
    "{'max_depth': 3, 'n_estimators': 15}\n",
    "accuracy score (train): 0.816000\n",
    "accuracy score (test): 0.816000\n",
    "\n",
    "AdaBoostClassifier :\n",
    "{'learning_rate': 1, 'n_estimators': 71}\n",
    "accuracy score (train): 0.829333\n",
    "accuracy score (test): 0.816000\n",
    "\n",
    "LogisticRegression :\n",
    "{}\n",
    "accuracy score (train): 0.809333\n",
    "accuracy score (test): 0.824000\n",
    "\n",
    "KNeighborsClassifier :\n",
    "{'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}\n",
    "accuracy score (train): 0.813333\n",
    "accuracy score (test): 0.816000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGE 2 and k=10 for KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier :\n",
    "{'criterion': 'entropy', 'max_depth': 5}\n",
    "accuracy score (train): 0.836000\n",
    "accuracy score (test): 0.812000\n",
    "\n",
    "RandomForestClassifier :\n",
    "{'max_depth': 4, 'n_estimators': 13}\n",
    "accuracy score (train): 0.822667\n",
    "accuracy score (test): 0.840000\n",
    "\n",
    "AdaBoostClassifier :\n",
    "{'learning_rate': 1, 'n_estimators': 26}\n",
    "accuracy score (train): 0.814667\n",
    "accuracy score (test): 0.800000\n",
    "\n",
    "LogisticRegression :\n",
    "{}\n",
    "accuracy score (train): 0.810667\n",
    "accuracy score (test): 0.820000\n",
    "\n",
    "KNeighborsClassifier :\n",
    "{'n_neighbors': 13, 'p': 1, 'weights': 'uniform'}\n",
    "accuracy score (train): 0.812000\n",
    "accuracy score (test): 0.812000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGE 2  and k=30 for KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier :\n",
    "{'criterion': 'entropy', 'max_depth': 3}\n",
    "accuracy score (train): 0.810667\n",
    "accuracy score (test): 0.828000\n",
    "\n",
    "RandomForestClassifier :\n",
    "{'max_depth': 4, 'n_estimators': 13}\n",
    "accuracy score (train): 0.821333\n",
    "accuracy score (test): 0.840000\n",
    "\n",
    "AdaBoostClassifier :\n",
    "{'learning_rate': 1, 'n_estimators': 71}\n",
    "accuracy score (train): 0.832000\n",
    "accuracy score (test): 0.804000\n",
    "\n",
    "LogisticRegression :\n",
    "{}\n",
    "accuracy score (train): 0.810667\n",
    "accuracy score (test): 0.820000\n",
    "\n",
    "KNeighborsClassifier :\n",
    "{'n_neighbors': 13, 'p': 1, 'weights': 'uniform'}\n",
    "accuracy score (train): 0.806667\n",
    "accuracy score (test): 0.820000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
